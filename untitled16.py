# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V07cmV4s46ClRUMT51nf2XycBqqTA8Dr
"""

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt


# ---------------------------------------------------------
# 1. SYNTHETIC MULTIVARIATE TIME-SERIES GENERATION
# ---------------------------------------------------------
def generate_synthetic_series(T=5000, n_features=5):
    t = np.linspace(0, 50, T)
    data = []

    for i in range(n_features):
        freq = np.random.uniform(0.5, 2.0)
        phase = np.random.uniform(0, np.pi)
        noise = np.random.normal(0, 0.1, size=T)

        series = np.sin(freq * t + phase) + 0.3 * np.sin(0.5 * t) + noise
        data.append(series)

    data = np.array(data).T
    return data


data = generate_synthetic_series()
print("Data shape:", data.shape)  # (T, n_features)


# ---------------------------------------------------------
# 2. DATASET CLASS
# ---------------------------------------------------------
class SeriesDataset(Dataset):
    def __init__(self, data, input_len=64, output_len=1):
        self.data = data
        self.input_len = input_len
        self.output_len = output_len

    def __len__(self):
        return len(self.data) - self.input_len - self.output_len

    def __getitem__(self, idx):
        x = self.data[idx: idx+self.input_len]
        y = self.data[idx+self.input_len : idx+self.input_len+self.output_len]
        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)


input_len = 64
output_len = 1

train_dataset = SeriesDataset(data[:4000], input_len, output_len)
test_dataset = SeriesDataset(data[4000:], input_len, output_len)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32)


# ---------------------------------------------------------
# 3. TRANSFORMER MODEL FOR FORECASTING
# ---------------------------------------------------------
class TransformerForecast(nn.Module):
    def __init__(self, feature_dim=5, embed_dim=32, n_heads=4, num_layers=2):
        super().__init__()

        self.embed = nn.Linear(feature_dim, embed_dim)
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=n_heads,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        self.fc_out = nn.Linear(embed_dim, feature_dim)

        # store attention weights
        self.attn_weights = None

    def forward(self, x):
        x = self.embed(x)

        def hook(module, input, output):
            self.attn_weights = module.self_attn.attn_output_weights.detach()

        for layer in self.encoder.layers:
            layer.register_forward_hook(hook)

        encoded = self.encoder(x)
        out = self.fc_out(encoded[:, -1, :])
        return out.unsqueeze(1)


model = TransformerForecast()
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=1e-3)


# ---------------------------------------------------------
# 4. TRAINING LOOP
# ---------------------------------------------------------
def train_model(model, loader):
    model.train()
    for x, y in loader:
        optimizer.zero_grad()
        pred = model(x)
        loss = criterion(pred, y)
        loss.backward()
        optimizer.step()
    return loss.item()


def evaluate(model, loader):
    model.eval()
    total = 0
    with torch.no_grad():
        for x, y in loader:
            pred = model(x)
            loss = criterion(pred, y)
            total += loss.item()
    return total / len(loader)


for epoch in range(10):
    train_loss = train_model(model, train_loader)
    test_loss = evaluate(model, test_loader)
    print(f"Epoch {epoch+1}: Train={train_loss:.4f} | Test={test_loss:.4f}")


# ---------------------------------------------------------
# 5. VISUALIZE ATTENTION WEIGHTS
# ---------------------------------------------------------
model.eval()
for x, y in test_loader:
    preds = model(x)
    weights = model.attn_weights[0].numpy()  # (heads, seq, seq)
    break

plt.imshow(weights[0], cmap='viridis')
plt.title("Attention Map - Head 1")
plt.colorbar()
plt.show()


# ---------------------------------------------------------
# 6. BASELINE LSTM FOR COMPARISON
# ---------------------------------------------------------
class LSTMForecast(nn.Module):
    def __init__(self, feature_dim=5, hidden=64):
        super().__init__()
        self.lstm = nn.LSTM(feature_dim, hidden, batch_first=True)
        self.fc = nn.Linear(hidden, feature_dim)

    def forward(self, x):
        out, _ = self.lstm(x)
        out = self.fc(out[:, -1, :])
        return out.unsqueeze(1)


baseline = LSTMForecast()
optim_lstm = optim.Adam(baseline.parameters(), lr=1e-3)


def train_lstm(model, loader):
    model.train()
    for x, y in loader:
        optim_lstm.zero_grad()
        pred = model(x)
        loss = criterion(pred, y)
        loss.backward()
        optim_lstm.step()


print("\nTraining LSTM baseline...")
for epoch in range(10):
    train_lstm(baseline, train_loader)

lstm_test_loss = evaluate(baseline, test_loader)
print("LSTM Test Loss:", lstm_test_loss)